{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFaj/IpdI88gTxRSQrYFU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik2411/NLP_Projects/blob/main/SentimentAnalysis-PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preprocessing***\n",
        "\n",
        "1.   Eliminate handles and URLs\n",
        "2.   Tokenize the string into words.\n",
        "3.   Remove stop words like \"and, is, a, on, etc.\"\n",
        "4.   Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.\n",
        "5.   Convert all your words to lower case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NkJMSxvscjEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujOmIs8dRl0b"
      },
      "outputs": [],
      "source": [
        "import nltk                                # Python library for NLP\n",
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import random                              # pseudo-random number generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads sample twitter dataset.\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9cibMjRdW0v",
        "outputId": "a35b182d-b41e-444d-9436-c3213b3f7147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "QFSjvUkSdaGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of positive tweets: ', len(all_positive_tweets))\n",
        "print('Number of negative tweets: ', len(all_negative_tweets))\n",
        "\n",
        "print('The type of all_positive_tweets is: ', type(all_positive_tweets))\n",
        "print('The type of all_negative_tweets is: ', type(all_negative_tweets))\n",
        "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n",
        "print('The type of a tweet entry is: ', type(all_positive_tweets[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRRPEK1adfZL",
        "outputId": "1801a8e6-7309-481f-d1b0-cf6d6e28d244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive tweets:  5000\n",
            "Number of negative tweets:  5000\n",
            "The type of all_positive_tweets is:  <class 'list'>\n",
            "The type of all_negative_tweets is:  <class 'list'>\n",
            "The type of a tweet entry is:  <class 'str'>\n",
            "The type of a tweet entry is:  <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preprocess raw text for Sentiment analysis***"
      ],
      "metadata": {
        "id": "YRScaEZ8eEyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Required packages for Preprocessing\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "metadata": {
        "id": "4d6jZNjSeR1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting a Tweet\n",
        "tweet = all_positive_tweets[1997]\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbslY2KMd-tm",
        "outputId": "856635f3-a038-4ee8-84a1-cbb902d5cffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@F_O_R_E_S_TBOY WEll if that was a heatwave, we've lowered our standards. :-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading Stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4UI25l6eA3K",
        "outputId": "13b070dc-9b65-4bb8-ff0d-43e7ead92826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[92m' + tweet)\n",
        "print('\\033[94m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4H7RueRecTF",
        "outputId": "82eb9ab0-d292-4fd6-fcdc-29e7959f3db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m@F_O_R_E_S_TBOY WEll if that was a heatwave, we've lowered our standards. :-)\n",
            "\u001b[94m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Remove hyperlinks, Twitter marks and styles***\n"
      ],
      "metadata": {
        "id": "dv8qr_jGggLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove old style retweet text \"RT\"\n",
        "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)"
      ],
      "metadata": {
        "id": "sykINUybeklj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WcihX7sAesKS",
        "outputId": "f8ab419a-8372-4e9d-b967-614c42ea5eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@F_O_R_E_S_TBOY WEll if that was a heatwave, we've lowered our standards. :-)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove hyperlinks\n",
        "tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)"
      ],
      "metadata": {
        "id": "z7rl9hRreteH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Part         | Meaning                                                                                                                                               |\n",
        "| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `http`       | Matches the literal characters `http`.                                                                                                                |\n",
        "| `s?`         | The `s` is **optional** (`?` means \"0 or 1 occurrences\"), so this allows for both `http` and `https`.                                                 |\n",
        "| `://`        | Matches the literal characters `://`.                                                                                                                 |\n",
        "| `[^\\s\\n\\r]+` | Matches one or more (`+`) characters that are **not** (`^`) `\\s` matches any whitespace character.whitespace, newline `\\n`, or carriage return `\\r`. This captures the **rest of the URL**. |\n"
      ],
      "metadata": {
        "id": "RtaiKnLffg2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E4QvYdgYgOPL",
        "outputId": "0909dd57-8a1a-4cb2-e595-d48a95bc839b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@F_O_R_E_S_TBOY WEll if that was a heatwave, we've lowered our standards. :-)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove hashtags\n",
        "# only removing the hash # sign from the word\n",
        "tweet2 = re.sub(r'#', '', tweet2)"
      ],
      "metadata": {
        "id": "JgLu86J6gF-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6s2lgLjGgKMJ",
        "outputId": "f98929e9-7839-4504-a1b8-55f8ba72f081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@F_O_R_E_S_TBOY WEll if that was a heatwave, we've lowered our standards. :-)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Tokenize the string***"
      ],
      "metadata": {
        "id": "5FapcYksgwXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)"
      ],
      "metadata": {
        "id": "dVUV18Uigv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize tweets\n",
        "tweet_tokens = tokenizer.tokenize(tweet2)"
      ],
      "metadata": {
        "id": "Ds9OSvvVgLq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnrCs9Avg4jx",
        "outputId": "7d450315-fbaf-4353-8862-191178886560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well',\n",
              " 'if',\n",
              " 'that',\n",
              " 'was',\n",
              " 'a',\n",
              " 'heatwave',\n",
              " ',',\n",
              " \"we've\",\n",
              " 'lowered',\n",
              " 'our',\n",
              " 'standards',\n",
              " '.',\n",
              " ':-)']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Remove stop words and punctuations***"
      ],
      "metadata": {
        "id": "m0ipA_TYg-hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "print(stopwords_english)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnrRW0wqg5ge",
        "outputId": "a5aca0d3-e8b6-4dd8-ac4a-cd04b83cc05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stop words and creating a new list\n",
        "tweets_clean = []\n",
        "\n",
        "for word in tweet_tokens: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        tweets_clean.append(word)"
      ],
      "metadata": {
        "id": "dCZivrGoiT1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQlhdawWi8As",
        "outputId": "a80e9297-b7e5-43c3-e1f6-ba629ebc799f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well', 'heatwave', 'lowered', 'standards', ':-)']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Stemming***\n",
        "\n",
        "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
        "\n",
        "Consider the words:\n",
        "\n",
        "learn\n",
        "learning\n",
        "learned\n",
        "learnt\n",
        "All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n",
        "\n",
        "happy\n",
        "happiness\n",
        "happier\n",
        "We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen"
      ],
      "metadata": {
        "id": "HK84sp8OkLiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate stemming class\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "zknhjdpzjUl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store the stems\n",
        "tweets_stem = []"
      ],
      "metadata": {
        "id": "bhtgRVkPkeuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tweets_clean:\n",
        "    stem_word = stemmer.stem(word)  # stemming word\n",
        "    tweets_stem.append(stem_word)  # append to the list"
      ],
      "metadata": {
        "id": "gRrbpJcvkiAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_stem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjQLhJEdkkWm",
        "outputId": "827808fb-fd9f-4518-f9d1-9b0b6ae2a97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well', 'heatwav', 'lower', 'standard', ':-)']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5JTFGIDlMlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}